
var documents = [{
    "id": 0,
    "url": "https://jhermann.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://jhermann.github.io/about/",
    "title": "üë§Ô∏è About Me",
    "body": "üìù J√ºrgen Hermann ¬∑ üìß jh@web. de ¬∑ üë®‚Äçüè´ Talks &amp; Presentations ¬∑ üß©¬†My¬†(Software)¬†Projects üêç A long long time ago, I started with Python 1. 5 at web. de,founded the MoinMoin wiki project shortly thereafter, and am nowenjoying the increased traction that Python recently gets fromdata science and machine learning.  This blog is powered by fastpages "
    }, {
    "id": 2,
    "url": "https://jhermann.github.io/categories/",
    "title": "üìÇÔ∏è Categories",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; ‚Ä¢ {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 3,
    "url": "https://jhermann.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 4,
    "url": "https://jhermann.github.io/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ ‚Äúsitemap. xml‚Äù   absolute_url }}   "
    }, {
    "id": 5,
    "url": "https://jhermann.github.io/linux/deployment/2020/03/14/fpm_effing_package_managers.html",
    "title": "Packaging Software with ‚Äòfpm‚Äô",
    "body": "2020/03/14 -            What it does : Basically, ‚Äòfpm‚Äô allows you to deploy any software via OS packages,from an installation tree on disk or from already built artifacts specific tothe chosen implementation language. The main advantage over native tooling is you do not need to knowabout every minute detail of the involved commands and metafile formatsfor every platform. In case of Debian, that is at least the ‚Äúcontrol‚Äù and ‚Äúrules‚Äù files,and tools like ‚Äúbuildpackage‚Äù and ‚Äúdebhelpers‚Äù. How to install it : ‚Äòfpm‚Äô is written in Ruby and can thus be installed via gem install. But you can also run it with JRuby in a JVM,that you might happen to already have on a host anyway. Using JRuny spares you the headache of juggling multiple Ruby versionsand isolating gems against other Ruby applications. If you package it up that way, it also can be installed on any Linux releasebecause you only need a Java8 JRE installed to run it‚Äì no native code involved. You can use the fpm. sh script in thepriscilla project on GitHubto package ‚Äòfpm‚Äô with itself. As written, it works for Debian derivatives,but should be adaptable to other distros with a few changes(remember, ‚Äòfpm‚Äô makes that easy). If you call . /fpm. sh pkg, the package contents is created inbuild as a staging area, and when everything is ready, fpm is called from within that staging area to create the final package: build/opt_tools_fpm/opt/tools/fpm/bin/fpm \  -s dir -t deb -n opt-tools-fpm -v 1. 11. 0 \  --iteration 1 --category tools \  --deb-user root --deb-group root \  -m ' Juergen Hermann  &lt;jh@web. de&gt;' \  --license 'See contained license, or homepage' \  --vendor github. com/jhermann/priscilla \  --description 'fpm helps you build packages quickly and easily' \  --url http://fpm. readthedocs. io/ \  --workdir $PWD/build/opt_tools_fpm/tmp \  -a all -d 'openjdk-8-jre|‚Ä¶|java8-runtime-headless' \  opt usrYes, this is a mouthful, but still shorter than a control or . spec file,and easily adapted to other package managers. This created a DEB file . /build/opt_tools_fpm/opt-tools-fpm_1. 11. 0-1_all. deb, with this metadata: new debian package, version 2. 0. size 23763108 bytes: control archive= 30446 bytes.   462 bytes,  12 lines   control        119039 bytes, 1118 lines   md5sums        Package: opt-tools-fpm Version: 1. 11. 0-1 License: See contained license, or homepage Vendor: github. com/jhermann/priscilla Architecture: all Maintainer:  Juergen Hermann  &lt;jh@web. de&gt; Installed-Size: 27046 Depends: openjdk-8-jre|zulu8|‚Ä¶|java8-runtime-headless Section: tools Priority: extra Homepage: http://fpm. readthedocs. io/ Description: fpm helps you build packages quickly and easilyThe major part of files is installed into /opt/tools/fpm,but a symlink at /usr/bin/fpm makes the command available on the path. Building fpm 1. 11. 0 that way was tested on Ubuntu Bionic usingopenjdk-8-jre-headless 8u242. How to use it : There are a lot of source and target types available in fpm(dir, gem, deb, npm, rpm, tar, cpan, pear, empty, puppet, python, osxpkg, solaris, p5p, pkgin, freebsd, apk, snap, pleaserun, zip, virtualenv, pacman, sh),this example converts a Python workdir into a DEB package file. The rudiments project serves as the example here, but you can use any pure Python project built with setuptools. You have to clone the project and then call fpm like so: ( deactivate 2&gt;/dev/null; py=/usr/bin/python3; \ fpm -s python -t deb --category python \  --python-bin $py \  --python-pip  $py -m pip  \  --python-package-name-prefix  $(basename $py)  \  --python-obey-requirements-txt \  --python-install-data  /usr/local/share/$(basename $py)/$($py . /setup. py --name)  \  -m  \ $($py . /setup. py --author)\  &lt;$($py . /setup. py --author-email)&gt;  \  --vendor  $($py . /setup. py --url | cut -f3-4 -d/)  \  --force $PWD/setup. py )The --force option overwrites an existing package file, so you can call the command multiple times without an error. If you inspect the built package with dpkg-deb -I python3-rudiments_*_all. deb,this is the output: new Debian package, version 2. 0. size 31062 bytes: control archive=1417 bytes.   366 bytes,  12 lines   control         3789 bytes,  30 lines   md5sums        Package: python3-rudiments Version: 0. 3. 1 License: Apache 2. 0 Vendor: github. com/jhermann Architecture: all Maintainer:  J√ºrgen Hermann  &lt;jh@web. de&gt; Installed-Size: 79 Depends: python3-requests (&gt;= 2. 6) Section: python Priority: extra Homepage: https://github. com/jhermann/rudiments Description: Rudiments ‚Äì Fundamental elements for any Python project. The package's content is placed into these directories: /usr/local/lib/python3. 6/usr/local/share/python3/rudiments/usr/share/doc/python3-rudimentsYou can install it using dpkg -i ‚Ä¶ and then do a quick import test with this command: python3 -c  import rudiments; print(rudiments) Where to go from here : Read more about ‚Äòfpm‚Äô on its github wiki, or watch this slide deck. Credits: Package icon by Breathe Icon Team: Sebastian Porta, Cory Kontros, Andrew Starr-Bochicchio "
    }, {
    "id": 6,
    "url": "https://jhermann.github.io/python/deployment/2020/03/08/ship_libs_with_shiv.html",
    "title": "Bundling Python Dependencies in a ZIP Archive",
    "body": "2020/03/08 -            The Basic Idea : If you have a set of Python scripts that are all using the same set of required packages, you can distribute those dependencies in the form of a zipapp, i. e. in a single executable file. See Building Zipapps (PEP 441) for details if you're new to the concept of zipped Python application bundles Unlike shipping a script in a virtualenv built within a single project, you can have a project for the base libraries and other projects for the scripts, including scripts written by end users who are just using your dependencies. You can also deploy any PyPI package that way, with a simple call of shiv, as shown in the next section using Pandas. A Practical Example : The following example uses the well-known Pandas data science library, but this works for any project built with setuptools or any other build tool creating Python packages that declare their requirements. So, to create your base library release artifact, install and call shiv like this: python3. 8 -m pip install --user shivpython3. 8 -m shiv -p '/usr/bin/python3. 8 -IS' \         -o ~/bin/_lib-pandas pandas==1. 0. 1Do this in a virtualenv and leave out the --user option if you want to keep your account's home directory clean. Note that we do not provide an entry point here, which means this zipapp drops into the given Python interpreter and is thus usable as an interpreter, with the contained packages available for import. Now we can exploit this to write a script using the zipapp as its interpreter: cat &gt;script &lt;&lt;'EOF'#! /usr/bin/env _lib-pandasimport reimport sysfrom pathlib import Pathimport pandas as pdprint('Using Pandas from',   Path(pd. __file__). parent. relative_to(Path. home()),   '\n\nPython path:')df = pd. DataFrame(sys. path, columns=['Path'])df. Path = df. Path. str. replace(f'^{ re. escape(str(Path. home())) }/', '~/')print(df)EOFchmod +x script. /scriptCalling the script produces the following output: Using Pandas from . shiv/_lib-pandas_23b2‚Ä¶d2/site-packages/pandas Python path:                        Path0                 ~/bin/_lib-pandas1               /usr/lib/python38. zip2                 /usr/lib/python3. 83           /usr/lib/python3. 8/lib-dynload4 ~/. shiv/_lib-pandas_23b2bb7d64c26139950435a64d. . . If you're familiar with Pandas, you'll instantly recognize the Python path output as coming from a Pandas data frame. üéâ This first execution is a bit slow on startup, because the cache directory you see at the end of the Python path has to be populated first. shiv's boot-strapping code unpacks extension packages containing native code into the file system, so the OS can load them. The underscore prefix in the zipapp name indicates this is not a command humans would normally use. Alternatively and especially in production you can deploy into e. g. /usr/local/lib/python3. 8/ and then use an absolute path instead of an env call as the script's interpreter. "
    }, {
    "id": 7,
    "url": "https://jhermann.github.io/python/deployment/2020/03/03/install_tools_with_dephell.html",
    "title": "Installing CLI Tools Using ‚Äòdephell‚Äô",
    "body": "2020/03/03 -            Introduction : ‚Äòdephell‚Äô is a useful add-on tool for project and venv management that works with existing standard tooling, instead of doing a bad replacement job like so many others. This post takes a look at how it can take over from pipsi (Python Script Installer, which is unmaintained) to manage isolated tool installations by providing each tool with its own virtual environment. Installation : Dephell is installed via a Python installer script into its own venv (compatible to what dephell itself creates as a so-called ‚Äòjail‚Äô). curl -L dephell. org/install | python3dephell needs at least Python 3. 6, which is the default on Ubuntu Bionic, so it just works‚Ñ¢ there. On Xenial, you need to install 3. 6+ from the Deadsnakes PPA first, and pipe the installer script into e. g. python3. 8. The only locations touched by the installer on a Posix host are ~/. local/bin/ and ~/. local/share/dephell/venvs/. Going into &#8216;jail&#8217; : As already mentioned, this post will take a deeper look into the dephell jail sub-command for venv management. Unlike pipsi, the former go-to tool for that purpose, it is maintained, supports full life-cycle management (i. e. it has a way to remove tool installations), and also supports projects that have several console entry points (i. e. expose more than one command). I also like it a lot more than pipx, which has a similar feature profile when compared to just dephell's jail sub-command, but YMMV. As a first example, to get rid of dephell again, just remove it using itself: dephell jail remove dephellNote that doing so leaves anything installed via dephell untouched (i. e. other jails still work), and reinstalling allows to manage those again. Adding more tools is done using jail install: dephell jail install shivshiv --versionMake sure that ~/. local/bin is in your PATH, which is not always the case on older GNU/Linux releases. You can easily list what you have installed: $ dephell jail list{  dephell : [   dephell  ],  shiv : [   shiv-info ,   shiv  ]}As you can see, the output is JSON by default and lists all installed tools with their possibly multiple entry points. You can add the --table option to get output more suited for humans. To see more details about a single venv, use jail show: $ dephell jail show dephell{  entrypoints : [   dephell  ],  name :  dephell ,  path :  /home/jhe/. local/share/dephell/venvs/dephell ,  size : {   lib :  43. 21Mb ,   total :  56. 78Mb  },  version :  0. 8. 1 }Finally, there is a jail try command to give new tools a quick spin in a temporary environment, without leaving any trace of it on your machine. $ dephell jail try --command  pip --version  pip‚Ä¶INFO running. . . pip 20. 0. 2 from /tmp/tmpnm5gvieo/lib/python3. 6/site-packages/pip (python 3. 6)Beyond &#8216;jail&#8217; : Besides jail, there are lots of other sub-commands for dependency management, handling docker images, creating common Python software project files, managing and vendoring your project's dependencies, and handling of project-specific venvs. See the full DepHell documentation for details on that. "
    }, {
    "id": 8,
    "url": "https://jhermann.github.io/python/deployment/2020/02/29/python_zippapps_on_windows.html",
    "title": "Enabling Easy Zipapp Installs on Windows",
    "body": "2020/02/29 -            Zipapps in a Nutshell : Zipapps are a way to distribute Python applicationsand all of their dependencies in a single binary file. This is comparable to statically linked golang apps or Java's ‚Äòexecutable JARs‚Äô. Their main advantage is that distributing and installing them is quite simple. Running Python code directly from ZIP archives is nothing new, PEP 273 made its debut in 2001, as part of Python 2. 3 in the form of the zipimport module. PEP 441 builds on this and describes mechanisms to bundle full applications into a single ZIP file that can be made executable. It was approved in 2015 and a first implementation appeared in Python 3. 5 via the zipapp module. See the PEP for details on how making a ZIP into an executable file works, but basically on POSIX systems the Python interpreter is called in a ‚Äòbang path‚Äô that is followed by the ZIP archive. The interpreter recognizes the ‚Äòscript‚Äô is a whole application archive and acts accordingly. On Windows, zipapps MUST carry the . pyz extension which is bound to the py wrapper command, which in turn looks at the bang path and calls a matching Python interpreter from the installed set. To display the bang path of a zipapp, use this command: python3 -m zipapp --info foo. pyzIf you want to change the requested Python version to one that is actually installed or that you prefer, change the bang path as part of the installation process: python3 -m zipapp -p '/usr/bin/env python3. 8' -o ~/bin/foo foo. pyzThis can also be done on an ad-hoc basis, by explicitly calling the desired interpreter: python3. 8 foo. pyz ‚Ä¶ # POSIXpy -3. 8 foo. pyz ‚Ä¶  # WindowsWell-known tools to build new zipapps, outside of the Python core, are pex (Twitter) and shiv (LinkedIn). See their documentation for details on bundling your own applications. Setting Up Windows 10 for Zipapps : On Windows, because there is no ‚Äò+x‚Äô flag, things are a bit more complicated than on POSIX. Zipapps MUST have a . pyz extension,for which the py launcher is registered as the default application. The net effect is that such files become executable and are handed over to the launcherif you add a few environment settings to your machine. In the user-specific environment settings, add a new PATHEXT variable(or extend an existing one), with the value %PATHEXT%;. PYZ. Also edit the PATH one and add a new %LOCALAPPDATA%\bin entry. Save everything (click ‚ÄúOK‚Äù), open a new command window, and verifythe changes with echo %PATHEXT% &amp; echo %PATH%Create the new bin directory by calling md %LOCALAPPDATA%\bin. Now you can place a zipapp file like foo. pyz in that directory,and it is immediately callable as foo. To get such a test subject, you can build shiv with itself: git clone https://github. com/linkedin/shiv. gitcd shivpy -3 -m venv --prompt shiv venvvenv\Scripts\activate. batpython -m pip install -e . shiv -e shiv. cli:main -o %LOCALAPPDATA%\bin\shiv. pyz . deactivateshiv --versionVariations : If that makes more sense to you, you can change the system-widevariables instead of the user-specific ones, and choose paths that areglobal for all users (like C:\usr\bin or similar). To make zipapps available network-wide, you can use %APPDATA% to store the zipapps,so you only have to maintain them once in case you regularlywork on several machines in the same network. Just make sure the same version of Python is used everywhere then. "
    }, {
    "id": 9,
    "url": "https://jhermann.github.io/linux/know-how/2020/02/28/env_with_arguments.html",
    "title": "Shell Scripts: env-shebang with Arguments",
    "body": "2020/02/28 -            The Problem : There is an old annoyance that, if you use env in a bang path to search the script interpreter in the shell's path, you cannot pass any arguments to it. Instead, all the text after the call to env is passed as one single argument, and env tries to find this as the executable to invoke, which fails of course when arguments are present. env is not the culprit here, but the very definition of how a bang path works (quoted from the bash manpage): If the program is a file beginning with #!, the remainder of the first line specifies an interpreter for the program. The shell executes the specified interpreter on operating systems that do not handle this executable format themselves. The arguments to the interpreter consist of a single optional argument following the interpreter name on the first line‚Ä¶ (emphasis mine) So what env gets to see in its argv array when you write something like #! /usr/bin/env python3 -I -S is ['/usr/bin/env', 'python3 -I -S']. And there is no python3 -I -S anywhere to be found that could interpret your script. üòû The Solution : The env command in coreutils 8. 30 solves this (i. e. Debian Buster only so far, Ubuntu Bionic still has 8. 28). The relevant change is introducing a split option (-S), designed to handle that special case of getting all arguments mushed together into one. In the example below, we want to pass the -I -S options to Python on startup. They increase security of a script, by reducing the possible ways an attacker can insert their malicious code into your runtime environment, as you can see from the help text: -I   : isolate Python from the user's environment (implies -E and -s)-E   : ignore PYTHON* environment variables (such as PYTHONPATH)-s   : don't add user site directory to sys. path; also PYTHONNOUSERSITE-S   : don't imply 'import site' on initializationYou can try the following yourself using docker run --rm -it --entrypoint /bin/bash python:3-slim-buster: $ cat &gt;isolated &lt;&lt;&#39;. &#39;#!/usr/bin/env -S python3 -I -Simport sysprint(&#39;\n&#39;. join(sys. path)). $ chmod +x isolated$ . /isolated/usr/local/lib/python38. zip/usr/local/lib/python3. 8/usr/local/lib/python3. 8/lib-dynloadNormally, the Python path would include both the current working directory (/ in this case) as well as site packages (/usr/local/lib/python3. 8/site-packages). However, we prevented their inclusion as a source of unanticipated code ‚Äì and you can be a happy cat again. üòª "
    }, {
    "id": 10,
    "url": "https://jhermann.github.io/tools/automation/2020/02/27/autoenv.html",
    "title": "Simplify Your Developer Life with `autoenv`",
    "body": "2020/02/27 -           When you work a lot with Python venvs,and thus have a lot of them sprinkled over your home directory,then remembering to activate the right one can be a source of problems,and is tedious at best. But there is a solution to automate that chore‚Äì that's what we have those boxes full of electronics for, after all. Meet autoenv : Consider this shell session and especially watch what happens to the prompt. jhe@workstation:~$ which python/usr/bin/pythonjhe@workstation:~$ cd src/github/rituals/(rituals)jhe@workstation:~/src/github/rituals$ which python/home/jhe/src/github/rituals/. venv/rituals/bin/python The magic wand was originally crafted by Kenneth Reitz,and can be found on GitHub. Installing autoenv : To get a working installation, the easiest way is to directly use a git checkout as follows: mkdir -p ~/. localtest -d ~/. local/autoenv \  || git clone  https://github. com/kennethreitz/autoenv. git  \         ~/. local/autoenvecho &gt;&gt;~/. bash_aliases  . ~/. local/autoenv/activate. sh . ~/. local/autoenv/activate. shThat's all there is to it. Now you just have to add a . env file to your project,like in this example. Security Considerations : If you're afraid that the cd command is wrapped by a bash function,the following shows that you need not fear,since that function isn't exported to any scripts you run. (rituals)jhe@workstation:~/src/github/rituals$ bash &lt;&lt;&lt; pwd; \  cd $PWD/. . /time-tunnel; pwd; which python /home/jhe/src/github/rituals/home/jhe/src/github/time-tunnel/home/jhe/src/github/rituals/. venv/rituals/bin/pythonThis pretty much restricts the modified cd to interactive use. Sub-shells behave differently, again that's what you'd expect working at the prompt. (rituals)jhe@workstation:~/src/github/rituals$ ( pwd; \  cd $PWD/. . /time-tunnel; pwd; which python )/home/jhe/src/github/rituals/home/jhe/src/github/time-tunnel/home/jhe/src/github/time-tunnel/. venv/time-tunnel/bin/pythonIf at any time you need the original command on the prompt, just use command cd ‚Ä¶ or builtin cd ‚Ä¶. Also, nobody can inject code into your shell just so, see what happens if we stumble overa new or modified . env file the first time: (rituals)jhe@workstation:~/src/github/rituals$ cd . (rituals)jhe@workstation:~/src/github/rituals$ echo &gt;&gt;. env(rituals)jhe@workstation:~/src/github/rituals$ cd . autoenv:autoenv: WARNING:autoenv: This is the first time you are about to source /home/jhe/src/github/rituals/. env:autoenv:autoenv:   --- (begin contents) ---------------------------------------autoenv:   # autoenv script (https://github. com/kennethreitz/autoenv)autoenv:   test \! -f . venv/$(basename $(pwd))/bin/activate || . . venv/$(basename $(pwd))/bin/activateautoenv:   autoenv:autoenv:   --- (end contents) -----------------------------------------autoenv:autoenv: Are you sure you want to allow this? (y/N) yNow you have all the information to decide whether this is something you'd like to use or not. I do, but YMMV. ü§î "
    }, {
    "id": 11,
    "url": "https://jhermann.github.io/python/deployment/2020/02/26/deadsnakes_on_debian.html",
    "title": "(Dead) Snakes on a‚Ä¶ Debian System",
    "body": "2020/02/26 -            The Deadsnakes PPA project originally built older Python releases for Ubuntu, so you could e. g. run unit tests on a new release using a Python version found on older releases (i. e. the ‚Äòdead‚Äô snakes). Nowadays, the project also builds newer Python versions ahead of what a certain release offers as its default. The packages contain the minor Python version in their name (e. g. python3. 6) and can thus be installed concurrently to the default python3 ones. Originally based on the Debian source packages, they can also be used on Debian and not just on Ubuntu. The build script and Dockerfile found here build packages for some Debian releases in their related Docker base images. Based on this, Python 3. 6 can be installed for all of Stretch, Buster, and Xenial, as a set of the usual core Python packages (python3. 6, python3. 6-venv, python3. 6-dev, ‚Ä¶). Note that Bionic comes with 3. 6 as a default. The same goes for Python 3. 7, with Buster having it as a default. Python 3. 8 is an add-on for all the (old-)stable releases (as of Feb 2020). Using this version makes the most sense to me, unless you have special needs forcing you to go to 3. 7 or 3. 6. Being an add-on everywhere ensures a similar experience regarding any quirks you encounter, and it is (right now) the newest stable version of Python. It also fits best what you get when using Docker's python:3-slim. "
    }, {
    "id": 12,
    "url": "https://jhermann.github.io/devops/continuous-delivery/2020/02/25/continuous-delivery.html",
    "title": "Continuous Delivery Explained",
    "body": "2020/02/25 -            I wrote this back in September 2014 and never published it, but since it's an introductory piece it stands its ground, so let this serve as an initial post‚Ä¶ CD in a Nutshell : A typical mission statement for Continuous Delivery is this‚Ä¶ Our highest priority is to satisfy the customer,through early and continuous delivery of valuable software. Continuous Delivery strives to improve the process of software delivery, by applying Continuous Deployment paired with automated testing and Continuous Integration. The goal is creating software developed to a high standard and easily packaged and deployed to test environments, resulting in the ability to rapidly, reliably and repeatedly push out enhancements and bug fixes to customers in small increments, at low risk and with minimal manual overhead. CD is effective because it facilitates an explorative approach by providing real, valuable measurements of the output of the process, and feeding those results back into the process. It's the next logical step after applying Agile principles to development, by expanding the scope to the whole software life-cycle and all involved parties, from inception to going live and then maintaining the product for a substantial amount of time in fast-paced iterations. Some More Details : Continuous Delivery means that your software is production-ready from day one of your project (even when it's not ‚Äúfeature complete‚Äù), and that you can release to users on demand at the push of a button. There are several practices and patterns that enable this, but the foundation is formed in particular by excellent configuration management, continuous integration, and comprehensive automated testing at all levels. The key pattern is the deployment pipeline, which is effectively the extension of continuous integration out to production, whereby every check-in produces a release candidate which is assessed for its fitness to be released to production through a series of automated and then manual tests. In order to be able to perform these validations against every build, your regression tests must be automated ‚Äî both at the unit and acceptance level. Humans then perform tasks such as exploratory testing, usability testing, and showcases as later validations against builds that have already passed the automated tests. Builds can be deployed automatically on demand to testing, staging and production environments by the people authorized to do so ‚Äî note that this means deployments are triggered by humans and performed by machines. Through these practices, teams can get fast feedback on whether the software being delivered is useful, reduce the risk of release, and achieve a much more predictable, reliable process for software delivery. The backbone of CD is a culture in which everybody, if somehow involved in the delivery process, collaborates throughout the life-cycle of the product ‚Äî developers, testers, infrastructure, operators, DBAs, managers, and customers alike. Where to Go From Here? : Here are some resources for diving deeper into the topic: Jez Humble's Blog ¬∑ Continuous DeliveryCD Foundation ‚Äì A Neutral Home for the Next Generation of Continuous Delivery Collaboration. IT Revolution DevOps BlogDevops Weekly Mailing List (by @garethr)Team Topologiesüëç Credits: Devops-toolchain "
    }, {
    "id": 13,
    "url": "https://jhermann.github.io/how-to/know-how/2020/02/22/talks+presentations.html",
    "title": "Talks & Presentations",
    "body": "2020/02/22 -            Slides : On Speakerdeck: JupyterHub and Jupyter Notebook ‚Äì A View Under the Hood (PyData S√ºdwest Meetup KA ¬∑ 2018-04-24)Document the Data ‚Äì Creating Reports Using Docs Tooling (PyData S√ºdwest Meetup KA ¬∑ 2018-06-13)DevOps Tool Bazaar ‚Äì dh-virtualenv, fpm, sentry. io (DevOps Karlsruhe Meetup ¬∑ 2018-02-20)Videos : "
    }, {
    "id": 14,
    "url": "https://jhermann.github.io/misc/development/2020/02/21/projects_guided_tour.html",
    "title": "A Guided Tour of My Projects",
    "body": "2020/02/21 -           üöß This article is work in progress, and is updated regularly with new content.  For Python Developers : The Springerle GitHub organization is a collection of cookiecutter project templates, with templates for these types of project: single-file scripts (py-minimal-script),fully equipped packages and applications (py-generic-project),DEB packaging of existing projects (dh-virtualenv-mold ¬∑ debianized-pypi-mold),and more. Fundamental elements for any Python project, like configuration handling, are in the rudiments package It is used for runtime support in the Springerle templates mentioned above. On the project automation side of things, rituals is a library of Invoke tasks that are needed again and again. The Springerle templates use it to add management tasks that are updateable via pip, independently of the template. dependency-check-py is a shim to easily install OWASP dependency-check-cli into Python projects, by adding it to your requirements. The collection of Jupyter notebooks in whats-new-in-python3 summarize the ‚ÄúWhat's new in Python3?‚Äù documentation, with live code in the notebooks. Right now, it is unfinished and work in progress. Data Science &amp; Jupyter : jupyter-by-example has learning resources and practical tips on how to use Jupyter notebooks for fun &amp; profit. The Today I Learned about Data Science‚Ä¶ wiki contains similar information, with an extended scope (beyond Jupyter). Software Design &amp; Architecture : In c4-notation I collect technical resources about using the C4 model for visualizing software architecture. This is so far rather small and unfinished. See also my Diagrams as Code wiki page for similar resources. Debian Packages : I'm a contributor to dh-virtualenv, and I use it for all my Debian packaging needs. Especially when it comes to deploying applications with lots of dependencies, or services needing tight integration with the host (i. e. systemd units). For pure command line tools, pex and shiv can be a better alternative. There are several projects using dh-virtualenv as the basic packaging tool: 1and1/debianized-jupyterhub packages JupyterHub, a multi-user server for Jupyter notebooks, It also comes with a Python3 kernel, populated with an extensive data science stack. 1and1/debianized-sentry puts all sentry. io 9. x services into one package using systemd as a supervisor, you just need to add a PostgreSQL databasse. devpi-enterprisey/debianized-devpi allows easy deployment of the devpi package repository and proxy. This doesn't get updated that often, it basically chugs along silently on my workstations, speeding up virtualenv creation and allowing off-line work. Note that these are typically built in a Docker container. See the For Python Developers section on how to easily roll your own projects of this type. The dput-webdav plugin then allows you to use dput to comfortably upload created packages to a WebDAV repository like Artifactory (BinTray). Also check out (Dead) Snakes on a‚Ä¶ Debian System for being able to install newer Python versions on all major Debian-like releases. Docker &amp; Kubernetes : dockyard offers basic Dockerfile templates and other Docker build helpers. It contains some experiments regarding Python base images, shows how to build Debian packages within a container for repeatable builds, and comes with extensive documentation also showing how to optimize your Dockerfiles. Miscellaneous : The README of awesome-python-talks is an opinionated list of videos related to Python, with a focus on training and gaining hands-on experience. The awesome-tech-talks repository is very similar, but about software development and general IT topics. confluencer contains a CLI tool that automates common Atlassian Confluence maintenance tasks and content publishing. There's no place like home‚Ä¶ ruby-slippers is my dotfiles repository (and I found ‚Äúdotfiles‚Äù way too boring as a name). It also includes setup scripts for installing a bunch of developer tools and packages. "
    }, {
    "id": 15,
    "url": "https://jhermann.github.io/how-to/fastpages/2020/02/20/fastpages-pitfalls.html",
    "title": "fastpages: Pitfalls, Tips & Tricks",
    "body": "2020/02/20 -           üÜï üìù This article is updated regularly with new information as it is discovered. Introduction : fastpages will automatically convert Jupyter Notebooks saved into the _notebooks directory as blog posts! You must save your notebook with the naming convention YYYY-MM-DD-*. ipynb. Otherwise the file's modification time is used, but that is not something you should rely on. See Writing Blog Posts With Jupyter for more details on special markup features. Things to Consider : Isolate the title, subtitle, and metadata into their own cell (the first one). Always quote your title and sub-title. Also quote any categories that are not purely alpha-numerical-with-hyphens. Add author information to your pages, so the Atom feed has it ‚Äì Adding authors to your Jekyll site. Categories are hierarchical and become part of the final URL ‚Äì so give them a reasonable order (from generic to specific), have a consistent scheme for the site as a whole, and do not change them after publishing, otherwise you'll break people's links. For dual-use images (referenced in notebook contents and metadata), place the image in the _notebooks folder (or a sub-folder), use the relative path there in your notebook, and prefix that path for the image: attribute with images/copied_from_nb/. Categories must be listed in a bracketed list ‚Äì you cannot make it multi-line. Do not use today's date in filenames if you plan to publish soonish ‚Äì timezone differences to the build machines will possibly make your article disappear from the generated index. (fixed in template)This is a bit subjective, but I recommend to get yourself a local installation of JupyterHub and use that exclusively as your ‚Äúfastpages IDE‚Äù ‚Äì that means just ignore plain markdown files and do everything with notebooks. How-Tos : Updating the fastpages Template : To get changes from upstream, check out your own copy of the template project. To catch up for the first time, find the Initial commit in your blog repository, then make sure there are no commits after the time of that commit in the template. If there are, find the last commit before that date (the last one you already have), and use its SHA for a range ¬´sha¬ª. . HEAD in the update procedure that follows, starting with the git diff. Now to catch up, when you git pull --ff-only in the template, there is a line starting with Updating and a SHA range. Call git diff ¬´sha-range¬ª &gt;/tmp/fastpages. patch. Change to your blog repository workdir (make sure you have a clean workdir with no pending changes) and apply that patch: patch -N -p1 &lt;/tmp/fastpages. patch. Look out for patch rejects and resolve any conflicts. Finally commit the template changes, ideally mentioning the SHA range in the commit message for record-keeping (example). Troubleshooting : Article is not in the Index : See above hint regarding dates in the (near) future, and timezone differences between localhost and the cloud. Index Entry is Missing Most Fields : When an entry appears mostly empty, there is some metadata problem: missing quotes, improper YAML syntax, etc. Backtrack your changes in the git history, and never change metadata in bulk, so you can isolate the problem. And wait for a successful build after each change. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}